{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/valid.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of motivational words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    tokens = dict()\n",
    "    tokens['.'] = '<PERIOD>'\n",
    "    tokens[','] = '<COMMA>'\n",
    "    tokens['\"'] = '<QUOTATION_MARK>'\n",
    "    tokens[';'] = '<SEMICOLON>'\n",
    "    tokens['!'] = '<EXCLAMATION_MARK>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['('] = '<LEFT_PAREN>'\n",
    "    tokens[')'] = '<RIGHT_PAREN>'\n",
    "    tokens['?'] = '<QUESTION_MARK>'\n",
    "    tokens['-'] = '<DASH>'\n",
    "    tokens['\\n'] = '<NEW_LINE>'\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "def preprocess_and_save_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    \n",
    "    input_file = os.path.join(dataset_path)\n",
    "    with open(input_file, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data('./data/valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids \n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    n_batches = len(words)//batch_size\n",
    "\n",
    "    words = words[:n_batches*batch_size]\n",
    "    y_len = len(words) - sequence_length\n",
    "    x, y = [], []\n",
    "    \n",
    "    for idx in range(0, y_len):\n",
    "        idx_end = sequence_length + idx\n",
    "        x_batch = words[idx:idx_end]\n",
    "        x.append(x_batch)\n",
    "        batch_y =  words[idx_end]\n",
    "        y.append(batch_y)    \n",
    "  \n",
    "    data = TensorDataset(torch.tensor(x), torch.tensor(y))\n",
    "   \n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module): \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5, lr=0.001):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "    \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "  \n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "    \n",
    "        out = out[:, -1]\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        inputs, target = inp.cuda(), target.cuda()\n",
    "\n",
    "   \n",
    "    output, h = rnn(inputs, h)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, scheduler, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "                \n",
    "        scheduler.step(loss)\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length =  15\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 300\n",
    "hidden_dim = 500\n",
    "n_layers = 2\n",
    "\n",
    "show_every_n_batches = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 7.160677425563335\n",
      "\n",
      "Epoch:    1/10    Loss: 6.58905853331089\n",
      "\n",
      "Epoch:    1/10    Loss: 6.423418145626783\n",
      "\n",
      "Epoch:    1/10    Loss: 6.286110181361437\n",
      "\n",
      "Epoch:    1/10    Loss: 6.133414402604103\n",
      "\n",
      "Epoch:    1/10    Loss: 6.090179368853569\n",
      "\n",
      "Epoch:    1/10    Loss: 6.0231430567801\n",
      "\n",
      "Epoch:    1/10    Loss: 5.9000828713178635\n",
      "\n",
      "Epoch:    1/10    Loss: 5.892559997737408\n",
      "\n",
      "Epoch:    1/10    Loss: 5.895261742174625\n",
      "\n",
      "Epoch:    1/10    Loss: 5.823074474930763\n",
      "\n",
      "Epoch:    1/10    Loss: 5.747437451034784\n",
      "\n",
      "Epoch:    1/10    Loss: 5.777952998876572\n",
      "\n",
      "Epoch:    1/10    Loss: 5.765991888940334\n",
      "\n",
      "Epoch:    1/10    Loss: 5.7267242930829525\n",
      "\n",
      "Epoch:    1/10    Loss: 5.714062798768282\n",
      "\n",
      "Epoch:    1/10    Loss: 5.673074822872877\n",
      "\n",
      "Epoch:    1/10    Loss: 5.713571436703205\n",
      "\n",
      "Epoch:    1/10    Loss: 5.670213121920824\n",
      "\n",
      "Epoch:    1/10    Loss: 5.646732442080975\n",
      "\n",
      "Epoch:    1/10    Loss: 5.637774582952261\n",
      "\n",
      "Epoch:    1/10    Loss: 5.61912290379405\n",
      "\n",
      "Epoch:    1/10    Loss: 5.628428533673286\n",
      "\n",
      "Epoch:    1/10    Loss: 5.626457326114178\n",
      "\n",
      "Epoch:    1/10    Loss: 5.614152893424034\n",
      "\n",
      "Epoch:    1/10    Loss: 5.549874693155289\n",
      "\n",
      "Epoch:    1/10    Loss: 5.610535811632872\n",
      "\n",
      "Epoch:    1/10    Loss: 5.5808597803115845\n",
      "\n",
      "Epoch:    1/10    Loss: 5.5610698983073235\n",
      "\n",
      "Epoch:    1/10    Loss: 5.54089979454875\n",
      "\n",
      "Epoch:    1/10    Loss: 5.512983188033104\n",
      "\n",
      "Epoch:    1/10    Loss: 5.539507031440735\n",
      "\n",
      "Epoch:    1/10    Loss: 5.49223380163312\n",
      "\n",
      "Epoch:    1/10    Loss: 5.525114446878433\n",
      "\n",
      "Epoch:    1/10    Loss: 5.4824389182031155\n",
      "\n",
      "Epoch:    1/10    Loss: 5.50837167724967\n",
      "\n",
      "Epoch:    1/10    Loss: 5.487002402544022\n",
      "\n",
      "Epoch:    1/10    Loss: 5.48552517965436\n",
      "\n",
      "Epoch:    1/10    Loss: 5.514230661094189\n",
      "\n",
      "Epoch:    1/10    Loss: 5.486761312931776\n",
      "\n",
      "Epoch:    1/10    Loss: 5.436470959335566\n",
      "\n",
      "Epoch:    1/10    Loss: 5.460584178566933\n",
      "\n",
      "Epoch:    1/10    Loss: 5.413726553320885\n",
      "\n",
      "Epoch:    1/10    Loss: 5.4379891231656075\n",
      "\n",
      "Epoch:    1/10    Loss: 5.467638865113258\n",
      "\n",
      "Epoch:    1/10    Loss: 5.439033802598715\n",
      "\n",
      "Epoch:    1/10    Loss: 5.41880526393652\n",
      "\n",
      "Epoch:    1/10    Loss: 5.421186089515686\n",
      "\n",
      "Epoch:    1/10    Loss: 5.350796971470118\n",
      "\n",
      "Epoch:    1/10    Loss: 5.425825208425522\n",
      "\n",
      "Epoch:    1/10    Loss: 5.387699481099844\n",
      "\n",
      "Epoch:    1/10    Loss: 5.396836154162884\n",
      "\n",
      "Epoch:    1/10    Loss: 5.354083605110645\n",
      "\n",
      "Epoch:    1/10    Loss: 5.356757082045078\n",
      "\n",
      "Epoch:    1/10    Loss: 5.4139956049621105\n",
      "\n",
      "Epoch:    1/10    Loss: 5.405314143747091\n",
      "\n",
      "Epoch:    1/10    Loss: 5.354360856115818\n",
      "\n",
      "Epoch:    1/10    Loss: 5.368621043860912\n",
      "\n",
      "Epoch:    1/10    Loss: 5.351655222475529\n",
      "\n",
      "Epoch:    1/10    Loss: 5.330431833863258\n",
      "\n",
      "Epoch:    1/10    Loss: 5.402819566428661\n",
      "\n",
      "Epoch:    1/10    Loss: 5.387866459786892\n",
      "\n",
      "Epoch:    1/10    Loss: 5.375474411994219\n",
      "\n",
      "Epoch:    1/10    Loss: 5.33726054802537\n",
      "\n",
      "Epoch:    1/10    Loss: 5.319529555737972\n",
      "\n",
      "Epoch:    1/10    Loss: 5.308320965617895\n",
      "\n",
      "Epoch:    1/10    Loss: 5.369787957519293\n",
      "\n",
      "Epoch:    1/10    Loss: 5.308959186077118\n",
      "\n",
      "Epoch:    1/10    Loss: 5.3025029599666595\n",
      "\n",
      "Epoch:    1/10    Loss: 5.324485138058662\n",
      "\n",
      "Epoch:    1/10    Loss: 5.30334435403347\n",
      "\n",
      "Epoch:    1/10    Loss: 5.339306574314833\n",
      "\n",
      "Epoch:    1/10    Loss: 5.30704952403903\n",
      "\n",
      "Epoch:    1/10    Loss: 5.301595874130726\n",
      "\n",
      "Epoch:    1/10    Loss: 5.276677697896957\n",
      "\n",
      "Epoch:    1/10    Loss: 5.309097893536091\n",
      "\n",
      "Epoch:    1/10    Loss: 5.258625276386738\n",
      "\n",
      "Epoch:    1/10    Loss: 5.310825493186712\n",
      "\n",
      "Epoch:    1/10    Loss: 5.300960142165422\n",
      "\n",
      "Epoch:    1/10    Loss: 5.276084750890732\n",
      "\n",
      "Epoch:    1/10    Loss: 5.2834024131298065\n",
      "\n",
      "Epoch:    1/10    Loss: 5.337973650544882\n",
      "\n",
      "Epoch:    1/10    Loss: 5.265196789056063\n",
      "\n",
      "Epoch:    1/10    Loss: 5.267314471304417\n",
      "\n",
      "Epoch:    1/10    Loss: 5.322701416909695\n",
      "\n",
      "Epoch:    1/10    Loss: 5.309215914458036\n",
      "\n",
      "Epoch:    2/10    Loss: 5.163441495461897\n",
      "\n",
      "Epoch:    2/10    Loss: 5.068767260760069\n",
      "\n",
      "Epoch:    2/10    Loss: 5.063193500041962\n",
      "\n",
      "Epoch:    2/10    Loss: 5.049084205180407\n",
      "\n",
      "Epoch:    2/10    Loss: 5.031870048493147\n",
      "\n",
      "Epoch:    2/10    Loss: 5.079627111554146\n",
      "\n",
      "Epoch:    2/10    Loss: 5.03538016974926\n",
      "\n",
      "Epoch:    2/10    Loss: 5.013212289661169\n",
      "\n",
      "Epoch:    2/10    Loss: 5.061385873705149\n",
      "\n",
      "Epoch:    2/10    Loss: 5.066147718578577\n",
      "\n",
      "Epoch:    2/10    Loss: 5.056829329580069\n",
      "\n",
      "Epoch:    2/10    Loss: 5.067982405424118\n",
      "\n",
      "Epoch:    2/10    Loss: 5.077722452580929\n",
      "\n",
      "Epoch:    2/10    Loss: 5.073944203555584\n",
      "\n",
      "Epoch:    2/10    Loss: 5.051345884799957\n",
      "\n",
      "Epoch:    2/10    Loss: 5.04431340098381\n",
      "\n",
      "Epoch:    2/10    Loss: 5.033352583646774\n",
      "\n",
      "Epoch:    2/10    Loss: 5.054433308541775\n",
      "\n",
      "Epoch:    2/10    Loss: 5.0050271563231945\n",
      "\n",
      "Epoch:    2/10    Loss: 5.042825222015381\n",
      "\n",
      "Epoch:    2/10    Loss: 5.041895631700754\n",
      "\n",
      "Epoch:    2/10    Loss: 5.060349654406309\n",
      "\n",
      "Epoch:    2/10    Loss: 5.040991876274347\n",
      "\n",
      "Epoch:    2/10    Loss: 4.988793700933456\n",
      "\n",
      "Epoch:    2/10    Loss: 5.00439951941371\n",
      "\n",
      "Epoch:    2/10    Loss: 5.004828419536352\n",
      "\n",
      "Epoch:    2/10    Loss: 5.011021427810192\n",
      "\n",
      "Epoch:    2/10    Loss: 5.057082217186689\n",
      "\n",
      "Epoch:    2/10    Loss: 5.042378626763821\n",
      "\n",
      "Epoch:    2/10    Loss: 5.054810028523207\n",
      "\n",
      "Epoch:    2/10    Loss: 5.046265069395304\n",
      "\n",
      "Epoch:    2/10    Loss: 4.980993669480085\n",
      "\n",
      "Epoch:    2/10    Loss: 5.071220811456442\n",
      "\n",
      "Epoch:    2/10    Loss: 4.989729378372431\n",
      "\n",
      "Epoch:    2/10    Loss: 5.039727922528982\n",
      "\n",
      "Epoch:    2/10    Loss: 5.033282972872257\n",
      "\n",
      "Epoch:    2/10    Loss: 5.034783881157637\n",
      "\n",
      "Epoch:    2/10    Loss: 5.038893599063158\n",
      "\n",
      "Epoch:    2/10    Loss: 5.069534998387098\n",
      "\n",
      "Epoch:    2/10    Loss: 5.019819799810648\n",
      "\n",
      "Epoch:    2/10    Loss: 4.992568697780371\n",
      "\n",
      "Epoch:    2/10    Loss: 5.035079304128885\n",
      "\n",
      "Epoch:    2/10    Loss: 5.0074572414159775\n",
      "\n",
      "Epoch:    2/10    Loss: 5.020515494048595\n",
      "\n",
      "Epoch:    2/10    Loss: 5.057861797511578\n",
      "\n",
      "Epoch:    2/10    Loss: 5.06790068000555\n",
      "\n",
      "Epoch:    2/10    Loss: 5.017526224255562\n",
      "\n",
      "Epoch:    2/10    Loss: 5.00513282045722\n",
      "\n",
      "Epoch:    2/10    Loss: 4.979113280773163\n",
      "\n",
      "Epoch:    2/10    Loss: 4.9957913644611835\n",
      "\n",
      "Epoch:    2/10    Loss: 5.030755463987589\n",
      "\n",
      "Epoch:    2/10    Loss: 5.011824693530798\n",
      "\n",
      "Epoch:    2/10    Loss: 5.044479493051767\n",
      "\n",
      "Epoch:    2/10    Loss: 5.01215536147356\n",
      "\n",
      "Epoch:    2/10    Loss: 5.048878505825996\n",
      "\n",
      "Epoch:    2/10    Loss: 5.009193405508995\n",
      "\n",
      "Epoch:    2/10    Loss: 5.038124706596136\n",
      "\n",
      "Epoch:    2/10    Loss: 5.020625188946724\n",
      "\n",
      "Epoch:    2/10    Loss: 5.041590664535761\n",
      "\n",
      "Epoch:    2/10    Loss: 5.00895868241787\n",
      "\n",
      "Epoch:    2/10    Loss: 5.071216855198145\n",
      "\n",
      "Epoch:    2/10    Loss: 5.02195043861866\n",
      "\n",
      "Epoch:    2/10    Loss: 4.974508218467236\n",
      "\n",
      "Epoch:    2/10    Loss: 5.0807472653687\n",
      "\n",
      "Epoch:    2/10    Loss: 5.045159999281168\n",
      "\n",
      "Epoch:    2/10    Loss: 5.060293018817902\n",
      "\n",
      "Epoch:    2/10    Loss: 4.984660990536213\n",
      "\n",
      "Epoch:    2/10    Loss: 5.044675413519144\n",
      "\n",
      "Epoch:    2/10    Loss: 5.02336261048913\n",
      "\n",
      "Epoch:    2/10    Loss: 4.977670565247536\n",
      "\n",
      "Epoch:    2/10    Loss: 5.05561962723732\n",
      "\n",
      "Epoch:    2/10    Loss: 4.971231196075678\n",
      "\n",
      "Epoch:    2/10    Loss: 5.035835988819599\n",
      "\n",
      "Epoch:    2/10    Loss: 5.040650028735399\n",
      "\n",
      "Epoch:    2/10    Loss: 5.016385436058044\n",
      "\n",
      "Epoch:    2/10    Loss: 5.017394006252289\n",
      "\n",
      "Epoch:    2/10    Loss: 5.023919600993395\n",
      "\n",
      "Epoch:    2/10    Loss: 5.062689483165741\n",
      "\n",
      "Epoch:    2/10    Loss: 5.064259439706802\n",
      "\n",
      "Epoch:    2/10    Loss: 5.075622316449881\n",
      "\n",
      "Epoch:    2/10    Loss: 5.057960189878941\n",
      "\n",
      "Epoch:    2/10    Loss: 5.068423714488745\n",
      "\n",
      "Epoch:    2/10    Loss: 5.024625815451145\n",
      "\n",
      "Epoch:    2/10    Loss: 5.042221255600452\n",
      "\n",
      "Epoch:    2/10    Loss: 4.994314678013325\n",
      "\n",
      "Epoch:    2/10    Loss: 5.004055693745613\n",
      "\n",
      "Epoch:    3/10    Loss: 4.8584302772175185\n",
      "\n",
      "Epoch:    3/10    Loss: 4.77867266908288\n",
      "\n",
      "Epoch:    3/10    Loss: 4.769690252840519\n",
      "\n",
      "Epoch:    3/10    Loss: 4.735527075827122\n",
      "\n",
      "Epoch:    3/10    Loss: 4.771314341574907\n",
      "\n",
      "Epoch:    3/10    Loss: 4.762059397995472\n",
      "\n",
      "Epoch:    3/10    Loss: 4.716166015714407\n",
      "\n",
      "Epoch:    3/10    Loss: 4.742429416626692\n",
      "\n",
      "Epoch:    3/10    Loss: 4.7908367440104485\n",
      "\n",
      "Epoch:    3/10    Loss: 4.749654043465853\n",
      "\n",
      "Epoch:    3/10    Loss: 4.732882667332888\n",
      "\n",
      "Epoch:    3/10    Loss: 4.767399124801159\n",
      "\n",
      "Epoch:    3/10    Loss: 4.785171344876289\n",
      "\n",
      "Epoch:    3/10    Loss: 4.746154047548771\n",
      "\n",
      "Epoch:    3/10    Loss: 4.746798003092408\n",
      "\n",
      "Epoch:    3/10    Loss: 4.7399067878723145\n",
      "\n",
      "Epoch:    3/10    Loss: 4.770211115479469\n",
      "\n",
      "Epoch:    3/10    Loss: 4.749191615730524\n",
      "\n",
      "Epoch:    3/10    Loss: 4.721599616110325\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    3/10    Loss: 4.749410767108202\n",
      "\n",
      "Epoch:    3/10    Loss: 4.753519654273987\n",
      "\n",
      "Epoch:    3/10    Loss: 4.761327393352985\n",
      "\n",
      "Epoch:    3/10    Loss: 4.788694716989994\n",
      "\n",
      "Epoch:    3/10    Loss: 4.741136495023966\n",
      "\n",
      "Epoch:    3/10    Loss: 4.765004076063633\n",
      "\n",
      "Epoch:    3/10    Loss: 4.791958769783378\n",
      "\n",
      "Epoch:    3/10    Loss: 4.760566491633654\n",
      "\n",
      "Epoch:    3/10    Loss: 4.798851903527975\n",
      "\n",
      "Epoch:    3/10    Loss: 4.783968899399042\n",
      "\n",
      "Epoch:    3/10    Loss: 4.747497692704201\n",
      "\n",
      "Epoch:    3/10    Loss: 4.789872378110886\n",
      "\n",
      "Epoch:    3/10    Loss: 4.82056056894362\n",
      "\n",
      "Epoch:    3/10    Loss: 4.7778035923838615\n",
      "\n",
      "Epoch:    3/10    Loss: 4.787312772125006\n",
      "\n",
      "Epoch:    3/10    Loss: 4.812111232429743\n",
      "\n",
      "Epoch:    3/10    Loss: 4.770889487117529\n",
      "\n",
      "Epoch:    3/10    Loss: 4.803953941911459\n",
      "\n",
      "Epoch:    3/10    Loss: 4.796065153554082\n",
      "\n",
      "Epoch:    3/10    Loss: 4.812932897359133\n",
      "\n",
      "Epoch:    3/10    Loss: 4.782657060772181\n",
      "\n",
      "Epoch:    3/10    Loss: 4.790665969252586\n",
      "\n",
      "Epoch:    3/10    Loss: 4.827365882694721\n",
      "\n",
      "Epoch:    3/10    Loss: 4.786077126860619\n",
      "\n",
      "Epoch:    3/10    Loss: 4.787077505141497\n",
      "\n",
      "Epoch:    3/10    Loss: 4.826094860211015\n",
      "\n",
      "Epoch:    3/10    Loss: 4.785442788153887\n",
      "\n",
      "Epoch:    3/10    Loss: 4.8078833520412445\n",
      "\n",
      "Epoch:    3/10    Loss: 4.83444594964385\n",
      "\n",
      "Epoch:    3/10    Loss: 4.770572744309902\n",
      "\n",
      "Epoch:    3/10    Loss: 4.80780465900898\n",
      "\n",
      "Epoch:    3/10    Loss: 4.790174067020416\n",
      "\n",
      "Epoch:    3/10    Loss: 4.792502775788307\n",
      "\n",
      "Epoch:    3/10    Loss: 4.850205238908529\n",
      "\n",
      "Epoch:    3/10    Loss: 4.779198594391346\n",
      "\n",
      "Epoch:    3/10    Loss: 4.835591293871403\n",
      "\n",
      "Epoch:    3/10    Loss: 4.791767347604036\n",
      "\n",
      "Epoch:    3/10    Loss: 4.8479240499436855\n",
      "\n",
      "Epoch:    3/10    Loss: 4.803375281393528\n",
      "\n",
      "Epoch:    3/10    Loss: 4.802153553813696\n",
      "\n",
      "Epoch:    3/10    Loss: 4.780030056834221\n",
      "\n",
      "Epoch:    3/10    Loss: 4.845921993255615\n",
      "\n",
      "Epoch:    3/10    Loss: 4.798702362924814\n",
      "\n",
      "Epoch:    3/10    Loss: 4.827746149152517\n",
      "\n",
      "Epoch:    3/10    Loss: 4.803480461239815\n",
      "\n",
      "Epoch:    3/10    Loss: 4.798094023019075\n",
      "\n",
      "Epoch:    3/10    Loss: 4.8337976932525635\n",
      "\n",
      "Epoch:    3/10    Loss: 4.856725163757801\n",
      "\n",
      "Epoch:    3/10    Loss: 4.795047737658024\n",
      "\n",
      "Epoch:    3/10    Loss: 4.78835916146636\n",
      "\n",
      "Epoch:    3/10    Loss: 4.8794446885585785\n",
      "\n",
      "Epoch:    3/10    Loss: 4.845158711075783\n",
      "\n",
      "Epoch:    3/10    Loss: 4.834078688174486\n",
      "\n",
      "Epoch:    3/10    Loss: 4.844061952084303\n",
      "\n",
      "Epoch:    3/10    Loss: 4.872748747467995\n",
      "\n",
      "Epoch:    3/10    Loss: 4.845013085752726\n",
      "\n",
      "Epoch:    3/10    Loss: 4.852173823863268\n",
      "\n",
      "Epoch:    3/10    Loss: 4.825093388557434\n",
      "\n",
      "Epoch:    3/10    Loss: 4.839122846722603\n",
      "\n",
      "Epoch:    3/10    Loss: 4.859799817204475\n",
      "\n",
      "Epoch:    3/10    Loss: 4.814054261893034\n",
      "\n",
      "Epoch:    3/10    Loss: 4.806439157575369\n",
      "\n",
      "Epoch:    3/10    Loss: 4.829137597233057\n",
      "\n",
      "Epoch:    3/10    Loss: 4.856444764882326\n",
      "\n",
      "Epoch:    3/10    Loss: 4.869453623890877\n",
      "\n",
      "Epoch:    3/10    Loss: 4.866759333759546\n",
      "\n",
      "Epoch:    3/10    Loss: 4.825679514557123\n",
      "\n",
      "Epoch:    4/10    Loss: 4.626748403364962\n",
      "\n",
      "Epoch:    4/10    Loss: 4.543316498398781\n",
      "\n",
      "Epoch:    4/10    Loss: 4.533998938277364\n",
      "\n",
      "Epoch:    4/10    Loss: 4.565893923863769\n",
      "\n",
      "Epoch:    4/10    Loss: 4.551536345854402\n",
      "\n",
      "Epoch:    4/10    Loss: 4.563205635175109\n",
      "\n",
      "Epoch:    4/10    Loss: 4.573169697076082\n",
      "\n",
      "Epoch:    4/10    Loss: 4.583334928378463\n",
      "\n",
      "Epoch:    4/10    Loss: 4.556415079161525\n",
      "\n",
      "Epoch:    4/10    Loss: 4.5930648148059845\n",
      "\n",
      "Epoch:    4/10    Loss: 4.529405429959297\n",
      "\n",
      "Epoch:    4/10    Loss: 4.583304796367884\n",
      "\n",
      "Epoch:    4/10    Loss: 4.553182972595096\n",
      "\n",
      "Epoch:    4/10    Loss: 4.560647863894701\n",
      "\n",
      "Epoch:    4/10    Loss: 4.572846902534366\n",
      "\n",
      "Epoch:    4/10    Loss: 4.52529420889914\n",
      "\n",
      "Epoch:    4/10    Loss: 4.610358798876405\n",
      "\n",
      "Epoch:    4/10    Loss: 4.578628657385707\n",
      "\n",
      "Epoch:    4/10    Loss: 4.5484213307499886\n",
      "\n",
      "Epoch:    4/10    Loss: 4.557457143440843\n",
      "\n",
      "Epoch:    4/10    Loss: 4.567353267222643\n",
      "\n",
      "Epoch:    4/10    Loss: 4.588358452543616\n",
      "\n",
      "Epoch:    4/10    Loss: 4.593268098309636\n",
      "\n",
      "Epoch:    4/10    Loss: 4.566446520388126\n",
      "\n",
      "Epoch:    4/10    Loss: 4.574038693681359\n",
      "\n",
      "Epoch:    4/10    Loss: 4.606874650344253\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6008666809648275\n",
      "\n",
      "Epoch:    4/10    Loss: 4.57098188996315\n",
      "\n",
      "Epoch:    4/10    Loss: 4.591218795627356\n",
      "\n",
      "Epoch:    4/10    Loss: 4.606330681592226\n",
      "\n",
      "Epoch:    4/10    Loss: 4.621954757720232\n",
      "\n",
      "Epoch:    4/10    Loss: 4.592686166986823\n",
      "\n",
      "Epoch:    4/10    Loss: 4.605509139597416\n",
      "\n",
      "Epoch:    4/10    Loss: 4.5896335653960705\n",
      "\n",
      "Epoch:    4/10    Loss: 4.621436160057783\n",
      "\n",
      "Epoch:    4/10    Loss: 4.599220657721162\n",
      "\n",
      "Epoch:    4/10    Loss: 4.638887895271182\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6446706261485815\n",
      "\n",
      "Epoch:    4/10    Loss: 4.62629652582109\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6173741817474365\n",
      "\n",
      "Epoch:    4/10    Loss: 4.621849820017815\n",
      "\n",
      "Epoch:    4/10    Loss: 4.639184143394232\n",
      "\n",
      "Epoch:    4/10    Loss: 4.653433561325073\n",
      "\n",
      "Epoch:    4/10    Loss: 4.623670073226094\n",
      "\n",
      "Epoch:    4/10    Loss: 4.585805702954531\n",
      "\n",
      "Epoch:    4/10    Loss: 4.626890141516924\n",
      "\n",
      "Epoch:    4/10    Loss: 4.606469418853521\n",
      "\n",
      "Epoch:    4/10    Loss: 4.669172549620271\n",
      "\n",
      "Epoch:    4/10    Loss: 4.631761614233255\n",
      "\n",
      "Epoch:    4/10    Loss: 4.659305281937122\n",
      "\n",
      "Epoch:    4/10    Loss: 4.641272462904453\n",
      "\n",
      "Epoch:    4/10    Loss: 4.646863661706448\n",
      "\n",
      "Epoch:    4/10    Loss: 4.633954901248217\n",
      "\n",
      "Epoch:    4/10    Loss: 4.626800313591957\n",
      "\n",
      "Epoch:    4/10    Loss: 4.64077365398407\n",
      "\n",
      "Epoch:    4/10    Loss: 4.663117313757539\n",
      "\n",
      "Epoch:    4/10    Loss: 4.633000660687685\n",
      "\n",
      "Epoch:    4/10    Loss: 4.678432330489159\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6832252237945795\n",
      "\n",
      "Epoch:    4/10    Loss: 4.669575367122889\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6385711915791035\n",
      "\n",
      "Epoch:    4/10    Loss: 4.669822711497545\n",
      "\n",
      "Epoch:    4/10    Loss: 4.67790063098073\n",
      "\n",
      "Epoch:    4/10    Loss: 4.657915337011218\n",
      "\n",
      "Epoch:    4/10    Loss: 4.680485827848315\n",
      "\n",
      "Epoch:    4/10    Loss: 4.653611596673727\n",
      "\n",
      "Epoch:    4/10    Loss: 4.659828569740057\n",
      "\n",
      "Epoch:    4/10    Loss: 4.687531191855669\n",
      "\n",
      "Epoch:    4/10    Loss: 4.637732090428472\n",
      "\n",
      "Epoch:    4/10    Loss: 4.682624189183116\n",
      "\n",
      "Epoch:    4/10    Loss: 4.6984281949698925\n",
      "\n",
      "Epoch:    4/10    Loss: 4.7086193561553955\n",
      "\n",
      "Epoch:    4/10    Loss: 4.688924957066774\n",
      "\n",
      "Epoch:    4/10    Loss: 4.680194191634655\n",
      "\n",
      "Epoch:    4/10    Loss: 4.725709840655327\n",
      "\n",
      "Epoch:    4/10    Loss: 4.661086354404688\n",
      "\n",
      "Epoch:    4/10    Loss: 4.708018451929092\n",
      "\n",
      "Epoch:    4/10    Loss: 4.720842910930514\n",
      "\n",
      "Epoch:    4/10    Loss: 4.700207598507404\n",
      "\n",
      "Epoch:    4/10    Loss: 4.730937626212835\n",
      "\n",
      "Epoch:    4/10    Loss: 4.682666216045618\n",
      "\n",
      "Epoch:    4/10    Loss: 4.659569777548313\n",
      "\n",
      "Epoch:    4/10    Loss: 4.734113434329629\n",
      "\n",
      "Epoch:    4/10    Loss: 4.707916244864464\n",
      "\n",
      "Epoch:    4/10    Loss: 4.699892468750477\n",
      "\n",
      "Epoch:    4/10    Loss: 4.726318206638098\n",
      "\n",
      "Epoch:    5/10    Loss: 4.492780438878319\n",
      "\n",
      "Epoch:    5/10    Loss: 4.3993653282523155\n",
      "\n",
      "Epoch:    5/10    Loss: 4.427991397678852\n",
      "\n",
      "Epoch:    5/10    Loss: 4.415694193914533\n",
      "\n",
      "Epoch:    5/10    Loss: 4.367751466110349\n",
      "\n",
      "Epoch:    5/10    Loss: 4.404065627604723\n",
      "\n",
      "Epoch:    5/10    Loss: 4.373949594795704\n",
      "\n",
      "Epoch:    5/10    Loss: 4.413584552705288\n",
      "\n",
      "Epoch:    5/10    Loss: 4.424732826650143\n",
      "\n",
      "Epoch:    5/10    Loss: 4.429425740614533\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4239182118326426\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4269029051065445\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4366771429777145\n",
      "\n",
      "Epoch:    5/10    Loss: 4.428971838206053\n",
      "\n",
      "Epoch:    5/10    Loss: 4.405258577316999\n",
      "\n",
      "Epoch:    5/10    Loss: 4.438890555873513\n",
      "\n",
      "Epoch:    5/10    Loss: 4.382090482860804\n",
      "\n",
      "Epoch:    5/10    Loss: 4.431713968515396\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4503561817109585\n",
      "\n",
      "Epoch:    5/10    Loss: 4.373988214880228\n",
      "\n",
      "Epoch:    5/10    Loss: 4.454204510897398\n",
      "\n",
      "Epoch:    5/10    Loss: 4.479226304218173\n",
      "\n",
      "Epoch:    5/10    Loss: 4.459853541105986\n",
      "\n",
      "Epoch:    5/10    Loss: 4.455140855163336\n",
      "\n",
      "Epoch:    5/10    Loss: 4.436569597572088\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4014091938734055\n",
      "\n",
      "Epoch:    5/10    Loss: 4.443417727947235\n",
      "\n",
      "Epoch:    5/10    Loss: 4.466312056407332\n",
      "\n",
      "Epoch:    5/10    Loss: 4.484326753765345\n",
      "\n",
      "Epoch:    5/10    Loss: 4.466898104175925\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4441354386508465\n",
      "\n",
      "Epoch:    5/10    Loss: 4.441688718274236\n",
      "\n",
      "Epoch:    5/10    Loss: 4.444916021078825\n",
      "\n",
      "Epoch:    5/10    Loss: 4.482807133346796\n",
      "\n",
      "Epoch:    5/10    Loss: 4.48845386877656\n",
      "\n",
      "Epoch:    5/10    Loss: 4.491902722045779\n",
      "\n",
      "Epoch:    5/10    Loss: 4.4299363773316145\n",
      "\n",
      "Epoch:    5/10    Loss: 4.443967347964644\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    5/10    Loss: 4.413279984146357\n",
      "\n",
      "Epoch:    5/10    Loss: 4.508048012852669\n",
      "\n",
      "Epoch:    5/10    Loss: 4.510725419968367\n",
      "\n",
      "Epoch:    5/10    Loss: 4.460140805691481\n",
      "\n",
      "Epoch:    5/10    Loss: 4.498106196522713\n",
      "\n",
      "Epoch:    5/10    Loss: 4.49301235191524\n",
      "\n",
      "Epoch:    5/10    Loss: 4.485750297084451\n",
      "\n",
      "Epoch:    5/10    Loss: 4.488588135689497\n",
      "\n",
      "Epoch:    5/10    Loss: 4.521628513932228\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5111196637153625\n",
      "\n",
      "Epoch:    5/10    Loss: 4.489472884684801\n",
      "\n",
      "Epoch:    5/10    Loss: 4.483102664351463\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5230230540037155\n",
      "\n",
      "Epoch:    5/10    Loss: 4.499276105314493\n",
      "\n",
      "Epoch:    5/10    Loss: 4.521032303571701\n",
      "\n",
      "Epoch:    5/10    Loss: 4.528125170618296\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5122016947716475\n",
      "\n",
      "Epoch:    5/10    Loss: 4.495102217420936\n",
      "\n",
      "Epoch:    5/10    Loss: 4.534401625394821\n",
      "\n",
      "Epoch:    5/10    Loss: 4.565460748970509\n",
      "\n",
      "Epoch:    5/10    Loss: 4.504198657348752\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5241385493427515\n",
      "\n",
      "Epoch:    5/10    Loss: 4.539779694750905\n",
      "\n",
      "Epoch:    5/10    Loss: 4.530459566041827\n",
      "\n",
      "Epoch:    5/10    Loss: 4.536325916647911\n",
      "\n",
      "Epoch:    5/10    Loss: 4.494464110583067\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5795464012771845\n",
      "\n",
      "Epoch:    5/10    Loss: 4.534891875460744\n",
      "\n",
      "Epoch:    5/10    Loss: 4.567740458995104\n",
      "\n",
      "Epoch:    5/10    Loss: 4.530429825186729\n",
      "\n",
      "Epoch:    5/10    Loss: 4.582556426525116\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5157059989869595\n",
      "\n",
      "Epoch:    5/10    Loss: 4.517340829595923\n",
      "\n",
      "Epoch:    5/10    Loss: 4.562229430302978\n",
      "\n",
      "Epoch:    5/10    Loss: 4.552174497395754\n",
      "\n",
      "Epoch:    5/10    Loss: 4.552299169823527\n",
      "\n",
      "Epoch:    5/10    Loss: 4.556392766535282\n",
      "\n",
      "Epoch:    5/10    Loss: 4.58875516615808\n",
      "\n",
      "Epoch:    5/10    Loss: 4.565879145637155\n",
      "\n",
      "Epoch:    5/10    Loss: 4.597694594413042\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5738712679594755\n",
      "\n",
      "Epoch:    5/10    Loss: 4.5399110317230225\n",
      "\n",
      "Epoch:    5/10    Loss: 4.601936722174287\n",
      "\n",
      "Epoch:    5/10    Loss: 4.616894118487835\n",
      "\n",
      "Epoch:    5/10    Loss: 4.597725428640842\n",
      "\n",
      "Epoch:    5/10    Loss: 4.613377675414085\n",
      "\n",
      "Epoch:    5/10    Loss: 4.580145217478275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.628268385306001\n",
      "\n",
      "Epoch:    6/10    Loss: 4.34955808384852\n",
      "\n",
      "Epoch:    6/10    Loss: 4.327777756378055\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2462278082966805\n",
      "\n",
      "Epoch:    6/10    Loss: 4.330608373507857\n",
      "\n",
      "Epoch:    6/10    Loss: 4.247101264074445\n",
      "\n",
      "Epoch:    6/10    Loss: 4.235729064792395\n",
      "\n",
      "Epoch:    6/10    Loss: 4.238572254776955\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2443306893110275\n",
      "\n",
      "Epoch:    6/10    Loss: 4.3061831779778\n",
      "\n",
      "Epoch:    6/10    Loss: 4.240400593727827\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2832017708569765\n",
      "\n",
      "Epoch:    6/10    Loss: 4.259146373718977\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2025470696389675\n",
      "\n",
      "Epoch:    6/10    Loss: 4.237865148112178\n",
      "\n",
      "Epoch:    6/10    Loss: 4.24219024553895\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2612793277949095\n",
      "\n",
      "Epoch:    6/10    Loss: 4.251272117719054\n",
      "\n",
      "Epoch:    6/10    Loss: 4.244007922708988\n",
      "\n",
      "Epoch:    6/10    Loss: 4.24143442325294\n",
      "\n",
      "Epoch:    6/10    Loss: 4.238027218729258\n",
      "\n",
      "Epoch:    6/10    Loss: 4.243701968342066\n",
      "\n",
      "Epoch:    6/10    Loss: 4.219150582328439\n",
      "\n",
      "Epoch:    6/10    Loss: 4.240887364372611\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2562384977936745\n",
      "\n",
      "Epoch:    6/10    Loss: 4.230719922110438\n",
      "\n",
      "Epoch:    6/10    Loss: 4.251725286245346\n",
      "\n",
      "Epoch:    6/10    Loss: 4.255053862929344\n",
      "\n",
      "Epoch:    6/10    Loss: 4.22402835637331\n",
      "\n",
      "Epoch:    6/10    Loss: 4.22180013731122\n",
      "\n",
      "Epoch:    6/10    Loss: 4.223109262064099\n",
      "\n",
      "Epoch:    6/10    Loss: 4.224944066256285\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2297673132270575\n",
      "\n",
      "Epoch:    6/10    Loss: 4.246279092505574\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2423145696520805\n",
      "\n",
      "Epoch:    6/10    Loss: 4.259446628391743\n",
      "\n",
      "Epoch:    6/10    Loss: 4.254108734428883\n",
      "\n",
      "Epoch:    6/10    Loss: 4.244054244831204\n",
      "\n",
      "Epoch:    6/10    Loss: 4.222631603479385\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2229418978095055\n",
      "\n",
      "Epoch:    6/10    Loss: 4.229896215721965\n",
      "\n",
      "Epoch:    6/10    Loss: 4.243354361504316\n",
      "\n",
      "Epoch:    6/10    Loss: 4.221837906166911\n",
      "\n",
      "Epoch:    6/10    Loss: 4.258338091894984\n",
      "\n",
      "Epoch:    6/10    Loss: 4.226166749373078\n",
      "\n",
      "Epoch:    6/10    Loss: 4.209887707605958\n",
      "\n",
      "Epoch:    6/10    Loss: 4.226894911378622\n",
      "\n",
      "Epoch:    6/10    Loss: 4.213438982143998\n",
      "\n",
      "Epoch:    6/10    Loss: 4.204392276704311\n",
      "\n",
      "Epoch:    6/10    Loss: 4.253869770094752\n",
      "\n",
      "Epoch:    6/10    Loss: 4.250736618414521\n",
      "\n",
      "Epoch:    6/10    Loss: 4.233975930139422\n",
      "\n",
      "Epoch:    6/10    Loss: 4.23514062166214\n",
      "\n",
      "Epoch:    6/10    Loss: 4.255933275446296\n",
      "\n",
      "Epoch:    6/10    Loss: 4.22864786349237\n",
      "\n",
      "Epoch:    6/10    Loss: 4.25333116017282\n",
      "\n",
      "Epoch:    6/10    Loss: 4.251480618491769\n",
      "\n",
      "Epoch:    6/10    Loss: 4.227312486618757\n",
      "\n",
      "Epoch:    6/10    Loss: 4.23570510558784\n",
      "\n",
      "Epoch:    6/10    Loss: 4.249728972092271\n",
      "\n",
      "Epoch:    6/10    Loss: 4.220552237704396\n",
      "\n",
      "Epoch:    6/10    Loss: 4.203409908339381\n",
      "\n",
      "Epoch:    6/10    Loss: 4.229955764487386\n",
      "\n",
      "Epoch:    6/10    Loss: 4.246355535462499\n",
      "\n",
      "Epoch:    6/10    Loss: 4.223911169916391\n",
      "\n",
      "Epoch:    6/10    Loss: 4.240396479144692\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2519306894391775\n",
      "\n",
      "Epoch:    6/10    Loss: 4.192537585273385\n",
      "\n",
      "Epoch:    6/10    Loss: 4.249216288328171\n",
      "\n",
      "Epoch:    6/10    Loss: 4.225571505725384\n",
      "\n",
      "Epoch:    6/10    Loss: 4.23036209307611\n",
      "\n",
      "Epoch:    6/10    Loss: 4.177651148289442\n",
      "\n",
      "Epoch:    6/10    Loss: 4.209993178024888\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2187088672071695\n",
      "\n",
      "Epoch:    6/10    Loss: 4.256131576374173\n",
      "\n",
      "Epoch:    6/10    Loss: 4.232623251155019\n",
      "\n",
      "Epoch:    6/10    Loss: 4.193677678704262\n",
      "\n",
      "Epoch:    6/10    Loss: 4.231883153319359\n",
      "\n",
      "Epoch:    6/10    Loss: 4.227827774360776\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2196737639606\n",
      "\n",
      "Epoch:    6/10    Loss: 4.224188352003694\n",
      "\n",
      "Epoch:    6/10    Loss: 4.2317675072699785\n",
      "\n",
      "Epoch:    6/10    Loss: 4.210668716579676\n",
      "\n",
      "Epoch:    6/10    Loss: 4.238206468522549\n",
      "\n",
      "Epoch:    6/10    Loss: 4.240039737895131\n",
      "\n",
      "Epoch:    6/10    Loss: 4.217917125672102\n",
      "\n",
      "Epoch:    6/10    Loss: 4.236519725993276\n",
      "\n",
      "Epoch:    7/10    Loss: 4.17699226195162\n",
      "\n",
      "Epoch:    7/10    Loss: 4.16274799220264\n",
      "\n",
      "Epoch:    7/10    Loss: 4.184230174869299\n",
      "\n",
      "Epoch:    7/10    Loss: 4.186628861352801\n",
      "\n",
      "Epoch:    7/10    Loss: 4.165695862844586\n",
      "\n",
      "Epoch:    7/10    Loss: 4.18892091140151\n",
      "\n",
      "Epoch:    7/10    Loss: 4.159130550920963\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1964460127055645\n",
      "\n",
      "Epoch:    7/10    Loss: 4.185291716828942\n",
      "\n",
      "Epoch:    7/10    Loss: 4.163743071258068\n",
      "\n",
      "Epoch:    7/10    Loss: 4.165147088468075\n",
      "\n",
      "Epoch:    7/10    Loss: 4.141640339046717\n",
      "\n",
      "Epoch:    7/10    Loss: 4.122107518836856\n",
      "\n",
      "Epoch:    7/10    Loss: 4.146039864048362\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1519919242709875\n",
      "\n",
      "Epoch:    7/10    Loss: 4.139089662581682\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1886581145226955\n",
      "\n",
      "Epoch:    7/10    Loss: 4.179881244897842\n",
      "\n",
      "Epoch:    7/10    Loss: 4.170155804604292\n",
      "\n",
      "Epoch:    7/10    Loss: 4.184721952304244\n",
      "\n",
      "Epoch:    7/10    Loss: 4.17459231428802\n",
      "\n",
      "Epoch:    7/10    Loss: 4.182672776281834\n",
      "\n",
      "Epoch:    7/10    Loss: 4.207277148962021\n",
      "\n",
      "Epoch:    7/10    Loss: 4.133637588471174\n",
      "\n",
      "Epoch:    7/10    Loss: 4.144986342638731\n",
      "\n",
      "Epoch:    7/10    Loss: 4.178693758323789\n",
      "\n",
      "Epoch:    7/10    Loss: 4.137100646272302\n",
      "\n",
      "Epoch:    7/10    Loss: 4.190435925498605\n",
      "\n",
      "Epoch:    7/10    Loss: 4.165812876075506\n",
      "\n",
      "Epoch:    7/10    Loss: 4.136808915063739\n",
      "\n",
      "Epoch:    7/10    Loss: 4.192254774272442\n",
      "\n",
      "Epoch:    7/10    Loss: 4.188791513442993\n",
      "\n",
      "Epoch:    7/10    Loss: 4.201066765934229\n",
      "\n",
      "Epoch:    7/10    Loss: 4.149353057146072\n",
      "\n",
      "Epoch:    7/10    Loss: 4.204019621014595\n",
      "\n",
      "Epoch:    7/10    Loss: 4.197646947577596\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1750741973519325\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1756869070231915\n",
      "\n",
      "Epoch:    7/10    Loss: 4.182413496077061\n",
      "\n",
      "Epoch:    7/10    Loss: 4.190317261964083\n",
      "\n",
      "Epoch:    7/10    Loss: 4.166893884539604\n",
      "\n",
      "Epoch:    7/10    Loss: 4.149769550189376\n",
      "\n",
      "Epoch:    7/10    Loss: 4.158629642799497\n",
      "\n",
      "Epoch:    7/10    Loss: 4.196671856567264\n",
      "\n",
      "Epoch:    7/10    Loss: 4.186360642313957\n",
      "\n",
      "Epoch:    7/10    Loss: 4.190963085740805\n",
      "\n",
      "Epoch:    7/10    Loss: 4.168633069843054\n",
      "\n",
      "Epoch:    7/10    Loss: 4.192249262705445\n",
      "\n",
      "Epoch:    7/10    Loss: 4.185276046395302\n",
      "\n",
      "Epoch:    7/10    Loss: 4.183544393628836\n",
      "\n",
      "Epoch:    7/10    Loss: 4.186506360769272\n",
      "\n",
      "Epoch:    7/10    Loss: 4.160016218200326\n",
      "\n",
      "Epoch:    7/10    Loss: 4.138677578419447\n",
      "\n",
      "Epoch:    7/10    Loss: 4.177666379138827\n",
      "\n",
      "Epoch:    7/10    Loss: 4.201687669381499\n",
      "\n",
      "Epoch:    7/10    Loss: 4.188101360574365\n",
      "\n",
      "Epoch:    7/10    Loss: 4.178777948021889\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    7/10    Loss: 4.175411881878972\n",
      "\n",
      "Epoch:    7/10    Loss: 4.203337583690882\n",
      "\n",
      "Epoch:    7/10    Loss: 4.184699207544327\n",
      "\n",
      "Epoch:    7/10    Loss: 4.208175951614976\n",
      "\n",
      "Epoch:    7/10    Loss: 4.182371204718947\n",
      "\n",
      "Epoch:    7/10    Loss: 4.182803416624665\n",
      "\n",
      "Epoch:    7/10    Loss: 4.139114225283265\n",
      "\n",
      "Epoch:    7/10    Loss: 4.175438638776541\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1294150948524475\n",
      "\n",
      "Epoch:    7/10    Loss: 4.20180775411427\n",
      "\n",
      "Epoch:    7/10    Loss: 4.197769373655319\n",
      "\n",
      "Epoch:    7/10    Loss: 4.205236809328198\n",
      "\n",
      "Epoch:    7/10    Loss: 4.211476178839803\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1576688550412655\n",
      "\n",
      "Epoch:    7/10    Loss: 4.140850808471441\n",
      "\n",
      "Epoch:    7/10    Loss: 4.186693279072642\n",
      "\n",
      "Epoch:    7/10    Loss: 4.204785607755184\n",
      "\n",
      "Epoch:    7/10    Loss: 4.217701710760593\n",
      "\n",
      "Epoch:    7/10    Loss: 4.203047664836049\n",
      "\n",
      "Epoch:    7/10    Loss: 4.214039519429207\n",
      "\n",
      "Epoch:    7/10    Loss: 4.180067304521799\n",
      "\n",
      "Epoch:    7/10    Loss: 4.178217390552163\n",
      "\n",
      "Epoch:    7/10    Loss: 4.191901374608278\n",
      "\n",
      "Epoch:    7/10    Loss: 4.1945661045610905\n",
      "\n",
      "Epoch:    7/10    Loss: 4.209965322166681\n",
      "\n",
      "Epoch:    7/10    Loss: 4.195245776325464\n",
      "\n",
      "Epoch:    7/10    Loss: 4.227783681824803\n",
      "\n",
      "Epoch:    7/10    Loss: 4.166527846828103\n",
      "\n",
      "Epoch:    7/10    Loss: 4.179592594504356\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1709104844114995\n",
      "\n",
      "Epoch:    8/10    Loss: 4.16110966168344\n",
      "\n",
      "Epoch:    8/10    Loss: 4.090518057346344\n",
      "\n",
      "Epoch:    8/10    Loss: 4.153592973947525\n",
      "\n",
      "Epoch:    8/10    Loss: 4.152300441637635\n",
      "\n",
      "Epoch:    8/10    Loss: 4.123159745708108\n",
      "\n",
      "Epoch:    8/10    Loss: 4.147412329912186\n",
      "\n",
      "Epoch:    8/10    Loss: 4.108635416254401\n",
      "\n",
      "Epoch:    8/10    Loss: 4.117246627807617\n",
      "\n",
      "Epoch:    8/10    Loss: 4.092021845281124\n",
      "\n",
      "Epoch:    8/10    Loss: 4.113445850089192\n",
      "\n",
      "Epoch:    8/10    Loss: 4.144856294617057\n",
      "\n",
      "Epoch:    8/10    Loss: 4.118187053129077\n",
      "\n",
      "Epoch:    8/10    Loss: 4.114217195659876\n",
      "\n",
      "Epoch:    8/10    Loss: 4.130429917946458\n",
      "\n",
      "Epoch:    8/10    Loss: 4.132039003074169\n",
      "\n",
      "Epoch:    8/10    Loss: 4.156058523803949\n",
      "\n",
      "Epoch:    8/10    Loss: 4.124416841194034\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1642268765717745\n",
      "\n",
      "Epoch:    8/10    Loss: 4.0971714202314615\n",
      "\n",
      "Epoch:    8/10    Loss: 4.148277083411813\n",
      "\n",
      "Epoch:    8/10    Loss: 4.150531260296702\n",
      "\n",
      "Epoch:    8/10    Loss: 4.135970529168844\n",
      "\n",
      "Epoch:    8/10    Loss: 4.103489585220814\n",
      "\n",
      "Epoch:    8/10    Loss: 4.130224961787462\n",
      "\n",
      "Epoch:    8/10    Loss: 4.10722959972918\n",
      "\n",
      "Epoch:    8/10    Loss: 4.134772514924407\n",
      "\n",
      "Epoch:    8/10    Loss: 4.135074898600578\n",
      "\n",
      "Epoch:    8/10    Loss: 4.171137357130647\n",
      "\n",
      "Epoch:    8/10    Loss: 4.11755346134305\n",
      "\n",
      "Epoch:    8/10    Loss: 4.182978976517916\n",
      "\n",
      "Epoch:    8/10    Loss: 4.119516083970666\n",
      "\n",
      "Epoch:    8/10    Loss: 4.12098066881299\n",
      "\n",
      "Epoch:    8/10    Loss: 4.128200216218829\n",
      "\n",
      "Epoch:    8/10    Loss: 4.144351726397872\n",
      "\n",
      "Epoch:    8/10    Loss: 4.116241866722703\n",
      "\n",
      "Epoch:    8/10    Loss: 4.117828318849206\n",
      "\n",
      "Epoch:    8/10    Loss: 4.136286694556475\n",
      "\n",
      "Epoch:    8/10    Loss: 4.115355191752315\n",
      "\n",
      "Epoch:    8/10    Loss: 4.141936549916863\n",
      "\n",
      "Epoch:    8/10    Loss: 4.167526649311185\n",
      "\n",
      "Epoch:    8/10    Loss: 4.159354319795966\n",
      "\n",
      "Epoch:    8/10    Loss: 4.155826512724161\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1318559143692255\n",
      "\n",
      "Epoch:    8/10    Loss: 4.154758755117655\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1501376032829285\n",
      "\n",
      "Epoch:    8/10    Loss: 4.121873276308179\n",
      "\n",
      "Epoch:    8/10    Loss: 4.126697864383459\n",
      "\n",
      "Epoch:    8/10    Loss: 4.147966431453824\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1572369039058685\n",
      "\n",
      "Epoch:    8/10    Loss: 4.144340481609106\n",
      "\n",
      "Epoch:    8/10    Loss: 4.178541107103229\n",
      "\n",
      "Epoch:    8/10    Loss: 4.160751523450017\n",
      "\n",
      "Epoch:    8/10    Loss: 4.137383576482534\n",
      "\n",
      "Epoch:    8/10    Loss: 4.147935755550861\n",
      "\n",
      "Epoch:    8/10    Loss: 4.103496013209224\n",
      "\n",
      "Epoch:    8/10    Loss: 4.130514377728105\n",
      "\n",
      "Epoch:    8/10    Loss: 4.168343802914023\n",
      "\n",
      "Epoch:    8/10    Loss: 4.151964131742716\n",
      "\n",
      "Epoch:    8/10    Loss: 4.143685577437282\n",
      "\n",
      "Epoch:    8/10    Loss: 4.14056777022779\n",
      "\n",
      "Epoch:    8/10    Loss: 4.151925778016448\n",
      "\n",
      "Epoch:    8/10    Loss: 4.147731529548764\n",
      "\n",
      "Epoch:    8/10    Loss: 4.154000008478761\n",
      "\n",
      "Epoch:    8/10    Loss: 4.124350111931562\n",
      "\n",
      "Epoch:    8/10    Loss: 4.144768685102463\n",
      "\n",
      "Epoch:    8/10    Loss: 4.14995701238513\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1639601439237595\n",
      "\n",
      "Epoch:    8/10    Loss: 4.154473207890987\n",
      "\n",
      "Epoch:    8/10    Loss: 4.144957078620791\n",
      "\n",
      "Epoch:    8/10    Loss: 4.178446343168616\n",
      "\n",
      "Epoch:    8/10    Loss: 4.151368672028184\n",
      "\n",
      "Epoch:    8/10    Loss: 4.126346241682768\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1371004357934\n",
      "\n",
      "Epoch:    8/10    Loss: 4.137381546199322\n",
      "\n",
      "Epoch:    8/10    Loss: 4.14474886469543\n",
      "\n",
      "Epoch:    8/10    Loss: 4.141702502965927\n",
      "\n",
      "Epoch:    8/10    Loss: 4.147353852167726\n",
      "\n",
      "Epoch:    8/10    Loss: 4.17786094173789\n",
      "\n",
      "Epoch:    8/10    Loss: 4.185521978884935\n",
      "\n",
      "Epoch:    8/10    Loss: 4.1717751286923885\n",
      "\n",
      "Epoch:    8/10    Loss: 4.155952235683799\n",
      "\n",
      "Epoch:    8/10    Loss: 4.165886459872127\n",
      "\n",
      "Epoch:    8/10    Loss: 4.109892599284649\n",
      "\n",
      "Epoch:    8/10    Loss: 4.178126148879528\n",
      "\n",
      "Epoch:    8/10    Loss: 4.201528325676918\n",
      "\n",
      "Epoch:    9/10    Loss: 4.064256505532698\n",
      "\n",
      "Epoch:    9/10    Loss: 4.09768027998507\n",
      "\n",
      "Epoch:    9/10    Loss: 4.117403523996472\n",
      "\n",
      "Epoch:    9/10    Loss: 4.089698513969779\n",
      "\n",
      "Epoch:    9/10    Loss: 4.094257067888975\n",
      "\n",
      "Epoch:    9/10    Loss: 4.100750423967838\n",
      "\n",
      "Epoch:    9/10    Loss: 4.1027800645679235\n",
      "\n",
      "Epoch:    9/10    Loss: 4.130339881405234\n",
      "\n",
      "Epoch:    9/10    Loss: 4.049738300964236\n",
      "\n",
      "Epoch:    9/10    Loss: 4.055598387494683\n",
      "\n",
      "Epoch:    9/10    Loss: 4.118766162544489\n",
      "\n",
      "Epoch:    9/10    Loss: 4.111753279343247\n",
      "\n",
      "Epoch:    9/10    Loss: 4.110720345750451\n",
      "\n",
      "Epoch:    9/10    Loss: 4.128790222108364\n",
      "\n",
      "Epoch:    9/10    Loss: 4.074622286483645\n",
      "\n",
      "Epoch:    9/10    Loss: 4.103410247713327\n",
      "\n",
      "Epoch:    9/10    Loss: 4.1152530908584595\n",
      "\n",
      "Epoch:    9/10    Loss: 4.102337211370468\n",
      "\n",
      "Epoch:    9/10    Loss: 4.0940754767507315\n",
      "\n",
      "Epoch:    9/10    Loss: 4.126418091356754\n",
      "\n",
      "Epoch:    9/10    Loss: 4.097772212699056\n",
      "\n",
      "Epoch:    9/10    Loss: 4.103672847151756\n",
      "\n",
      "Epoch:    9/10    Loss: 4.105900295078754\n",
      "\n",
      "Epoch:    9/10    Loss: 4.107630699872971\n",
      "\n",
      "Epoch:    9/10    Loss: 4.094347182661295\n",
      "\n",
      "Epoch:    9/10    Loss: 4.12946760840714\n",
      "\n",
      "Epoch:    9/10    Loss: 4.064722081646323\n",
      "\n",
      "Epoch:    9/10    Loss: 4.0964602287858725\n",
      "\n",
      "Epoch:    9/10    Loss: 4.100423512980342\n",
      "\n",
      "Epoch:    9/10    Loss: 4.140830991789699\n",
      "\n",
      "Epoch:    9/10    Loss: 4.099152393639088\n",
      "\n",
      "Epoch:    9/10    Loss: 4.104953138157725\n",
      "\n",
      "Epoch:    9/10    Loss: 4.100631609559059\n",
      "\n",
      "Epoch:    9/10    Loss: 4.138936450704932\n",
      "\n",
      "Epoch:    9/10    Loss: 4.059790087863803\n",
      "\n",
      "Epoch:    9/10    Loss: 4.131253069266677\n",
      "\n",
      "Epoch:    9/10    Loss: 4.087140738964081\n",
      "\n",
      "Epoch:    9/10    Loss: 4.098328094929457\n",
      "\n",
      "Epoch:    9/10    Loss: 4.144226647913456\n",
      "\n",
      "Epoch:    9/10    Loss: 4.101361863315105\n",
      "\n",
      "Epoch:    9/10    Loss: 4.102889159694314\n",
      "\n",
      "Epoch:    9/10    Loss: 4.087500238791108\n",
      "\n",
      "Epoch:    9/10    Loss: 4.083249904215336\n",
      "\n",
      "Epoch:    9/10    Loss: 4.133887991309166\n",
      "\n",
      "Epoch:    9/10    Loss: 4.121690526604652\n",
      "\n",
      "Epoch:    9/10    Loss: 4.132425129413605\n",
      "\n",
      "Epoch:    9/10    Loss: 4.110740980133414\n",
      "\n",
      "Epoch:    9/10    Loss: 4.147141996771097\n",
      "\n",
      "Epoch:    9/10    Loss: 4.092648470774293\n",
      "\n",
      "Epoch:    9/10    Loss: 4.072972165420651\n",
      "\n",
      "Epoch:    9/10    Loss: 4.11432839743793\n",
      "\n",
      "Epoch:    9/10    Loss: 4.0911700669676065\n",
      "\n",
      "Epoch:    9/10    Loss: 4.11080064997077\n",
      "\n",
      "Epoch:    9/10    Loss: 4.098291266709566\n",
      "\n",
      "Epoch:    9/10    Loss: 4.0901542361825705\n",
      "\n",
      "Epoch:    9/10    Loss: 4.124900236725807\n",
      "\n",
      "Epoch:    9/10    Loss: 4.166139030829072\n",
      "\n",
      "Epoch:    9/10    Loss: 4.116595244035125\n",
      "\n",
      "Epoch:    9/10    Loss: 4.121672708541155\n",
      "\n",
      "Epoch:    9/10    Loss: 4.083633428439498\n",
      "\n",
      "Epoch:    9/10    Loss: 4.110267156735063\n",
      "\n",
      "Epoch:    9/10    Loss: 4.13980570808053\n",
      "\n",
      "Epoch:    9/10    Loss: 4.120949322357774\n",
      "\n",
      "Epoch:    9/10    Loss: 4.150964215397835\n",
      "\n",
      "Epoch:    9/10    Loss: 4.139862904325128\n",
      "\n",
      "Epoch:    9/10    Loss: 4.124496577307582\n",
      "\n",
      "Epoch:    9/10    Loss: 4.067162171006203\n",
      "\n",
      "Epoch:    9/10    Loss: 4.09140564315021\n",
      "\n",
      "Epoch:    9/10    Loss: 4.126589186489582\n",
      "\n",
      "Epoch:    9/10    Loss: 4.134846584871411\n",
      "\n",
      "Epoch:    9/10    Loss: 4.117890380322933\n",
      "\n",
      "Epoch:    9/10    Loss: 4.124903507530689\n",
      "\n",
      "Epoch:    9/10    Loss: 4.137267215177417\n",
      "\n",
      "Epoch:    9/10    Loss: 4.134943982586265\n",
      "\n",
      "Epoch:    9/10    Loss: 4.145686844363809\n",
      "\n",
      "Epoch:    9/10    Loss: 4.111607758328319\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    9/10    Loss: 4.086771881207824\n",
      "\n",
      "Epoch:    9/10    Loss: 4.078895820304751\n",
      "\n",
      "Epoch:    9/10    Loss: 4.107445999979973\n",
      "\n",
      "Epoch:    9/10    Loss: 4.086273076012731\n",
      "\n",
      "Epoch:    9/10    Loss: 4.119758106768131\n",
      "\n",
      "Epoch:    9/10    Loss: 4.116138853132725\n",
      "\n",
      "Epoch:    9/10    Loss: 4.113968765363097\n",
      "\n",
      "Epoch:    9/10    Loss: 4.105491017922759\n",
      "\n",
      "Epoch:    9/10    Loss: 4.1793663538992405\n",
      "\n",
      "Epoch:    9/10    Loss: 4.1355050932615995\n",
      "\n",
      "Epoch:   10/10    Loss: 4.077266871929169\n",
      "\n",
      "Epoch:   10/10    Loss: 4.058593090623617\n",
      "\n",
      "Epoch:   10/10    Loss: 4.078632375225425\n",
      "\n",
      "Epoch:   10/10    Loss: 4.066049695014954\n",
      "\n",
      "Epoch:   10/10    Loss: 4.046324973925948\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0410935543477535\n",
      "\n",
      "Epoch:   10/10    Loss: 4.070519428700209\n",
      "\n",
      "Epoch:   10/10    Loss: 4.080893551930785\n",
      "\n",
      "Epoch:   10/10    Loss: 4.067364927381277\n",
      "\n",
      "Epoch:   10/10    Loss: 4.074931710958481\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0625165943056345\n",
      "\n",
      "Epoch:   10/10    Loss: 4.105835657566786\n",
      "\n",
      "Epoch:   10/10    Loss: 4.058726489543915\n",
      "\n",
      "Epoch:   10/10    Loss: 4.039798833429813\n",
      "\n",
      "Epoch:   10/10    Loss: 4.068312454968691\n",
      "\n",
      "Epoch:   10/10    Loss: 4.059360856190324\n",
      "\n",
      "Epoch:   10/10    Loss: 4.060696503147483\n",
      "\n",
      "Epoch:   10/10    Loss: 4.05805055424571\n",
      "\n",
      "Epoch:   10/10    Loss: 4.044790133833885\n",
      "\n",
      "Epoch:   10/10    Loss: 4.065539550036192\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0203722305595875\n",
      "\n",
      "Epoch:   10/10    Loss: 4.086043847724795\n",
      "\n",
      "Epoch:   10/10    Loss: 4.091929342597723\n",
      "\n",
      "Epoch:   10/10    Loss: 4.039350254461169\n",
      "\n",
      "Epoch:   10/10    Loss: 4.083178356289864\n",
      "\n",
      "Epoch:   10/10    Loss: 4.067853635177016\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0780934151262045\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0550286918878555\n",
      "\n",
      "Epoch:   10/10    Loss: 4.099129766225815\n",
      "\n",
      "Epoch:   10/10    Loss: 4.074255373328924\n",
      "\n",
      "Epoch:   10/10    Loss: 4.066619815304875\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0604493245482445\n",
      "\n",
      "Epoch:   10/10    Loss: 4.1012758910655975\n",
      "\n",
      "Epoch:   10/10    Loss: 4.061699705198407\n",
      "\n",
      "Epoch:   10/10    Loss: 4.080094983801246\n",
      "\n",
      "Epoch:   10/10    Loss: 4.060922011733055\n",
      "\n",
      "Epoch:   10/10    Loss: 4.075729999691248\n",
      "\n",
      "Epoch:   10/10    Loss: 4.071583613753319\n",
      "\n",
      "Epoch:   10/10    Loss: 4.063500240445137\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0854506976902485\n",
      "\n",
      "Epoch:   10/10    Loss: 4.100731432437897\n",
      "\n",
      "Epoch:   10/10    Loss: 4.090123236179352\n",
      "\n",
      "Epoch:   10/10    Loss: 4.062811965122819\n",
      "\n",
      "Epoch:   10/10    Loss: 4.115186246111989\n",
      "\n",
      "Epoch:   10/10    Loss: 4.086036650463939\n",
      "\n",
      "Epoch:   10/10    Loss: 4.060789106413722\n",
      "\n",
      "Epoch:   10/10    Loss: 4.07626529969275\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0986756682395935\n",
      "\n",
      "Epoch:   10/10    Loss: 4.109467940405011\n",
      "\n",
      "Epoch:   10/10    Loss: 4.114491792395711\n",
      "\n",
      "Epoch:   10/10    Loss: 4.085791703313589\n",
      "\n",
      "Epoch:   10/10    Loss: 4.130355771631002\n",
      "\n",
      "Epoch:   10/10    Loss: 4.098593346774578\n",
      "\n",
      "Epoch:   10/10    Loss: 4.090402700006962\n",
      "\n",
      "Epoch:   10/10    Loss: 4.090273719280958\n",
      "\n",
      "Epoch:   10/10    Loss: 4.070056492462754\n",
      "\n",
      "Epoch:   10/10    Loss: 4.08331935852766\n",
      "\n",
      "Epoch:   10/10    Loss: 4.124165354296565\n",
      "\n",
      "Epoch:   10/10    Loss: 4.098857199773192\n",
      "\n",
      "Epoch:   10/10    Loss: 4.041546242311597\n",
      "\n",
      "Epoch:   10/10    Loss: 4.109991654753685\n",
      "\n",
      "Epoch:   10/10    Loss: 4.061343180015683\n",
      "\n",
      "Epoch:   10/10    Loss: 4.109634457156062\n",
      "\n",
      "Epoch:   10/10    Loss: 4.0733664240688086\n",
      "\n",
      "Epoch:   10/10    Loss: 4.063009429723024\n",
      "\n",
      "Epoch:   10/10    Loss: 4.095258265733719\n",
      "\n",
      "Epoch:   10/10    Loss: 4.103924173861742\n",
      "\n",
      "Epoch:   10/10    Loss: 4.067771119996905\n",
      "\n",
      "Epoch:   10/10    Loss: 4.077462945133448\n",
      "\n",
      "Epoch:   10/10    Loss: 4.148196863010526\n",
      "\n",
      "Epoch:   10/10    Loss: 4.117510786280036\n",
      "\n",
      "Epoch:   10/10    Loss: 4.104687245562673\n",
      "\n",
      "Epoch:   10/10    Loss: 4.07510999776423\n",
      "\n",
      "Epoch:   10/10    Loss: 4.084013817831874\n",
      "\n",
      "Epoch:   10/10    Loss: 4.109596844762564\n",
      "\n",
      "Epoch:   10/10    Loss: 4.055981630459428\n",
      "\n",
      "Epoch:   10/10    Loss: 4.093004381284118\n",
      "\n",
      "Epoch:   10/10    Loss: 4.112222163006663\n",
      "\n",
      "Epoch:   10/10    Loss: 4.1341481022536755\n",
      "\n",
      "Epoch:   10/10    Loss: 4.11364714987576\n",
      "\n",
      "Epoch:   10/10    Loss: 4.083409456536174\n",
      "\n",
      "Epoch:   10/10    Loss: 4.088318690657616\n",
      "\n",
      "Epoch:   10/10    Loss: 4.071175299584866\n",
      "\n",
      "Epoch:   10/10    Loss: 4.106403989717364\n",
      "\n",
      "Epoch:   10/10    Loss: 4.059605879709125\n",
      "\n",
      "Epoch:   10/10    Loss: 4.131421405822039\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, scheduler, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fear and courage of all morality.\n",
      "the greatest discovery of life is that the future is the best remedy for it.\n",
      "the world belongs to the energetic and the brunt of the mind.\n",
      "the most difficult deception is to be a musician and a person of action.\n",
      "i think it's a great deal that the world is the person with your own life.\n",
      "the greatest discovery of the human mind is that it has been a limitation to the top of our lives.\n",
      "the most important thing about\n"
     ]
    }
   ],
   "source": [
    "gen_length = 100 \n",
    "prime_word = \"the\" \n",
    "sequence_length =  15\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "pad_word = '<PAD>'\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
